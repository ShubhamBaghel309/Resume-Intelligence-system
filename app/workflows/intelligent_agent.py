# app/workflows/intelligent_agent.py
from typing import TypedDict, Annotated, Literal
from langgraph.graph import StateGraph, END
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
import os

# Import existing components
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from querying.hybrid_search import HybridResumeSearch
from generation.answer_generation import generate_answer
import sqlite3


# ============= State Definition =============
class AgentState(TypedDict):
    """State that flows through the agent graph"""

    query: str  # User's original question
    query_analysis: dict  # LLM's analysis of the query
    search_strategy: str  # Chosen strategy: "sql", "vector", "hybrid"
    sql_filters: dict  # Extracted SQL filters
    vector_query: str  # Reformulated query for vector search
    candidate_ids: list  # Resume IDs from SQL filtering
    search_results: list  # Raw search results
    final_results: list  # Deduplicated and enriched results
    answer: str  # Generated natural language answer
    should_retry: bool  # Whether to retry with different strategy
    retry_count: int  # Number of retries attempted
    use_llm_sql: bool  # Flag to use LLM-generated SQL on retry
    llm_generated_sql: str  # SQL query generated by LLM
    chat_history: list[dict[str, any]]


# ============= Query Analysis Models =============
class QueryAnalysis(BaseModel):
    """Structured output from query analyzer"""

    query_type: Literal[
        "name_based",
        "skill_based",
        "experience_based",
        "project_based",
        "education_based",
        "location_based",
        "complex_multi_criteria",
    ] = Field(description="Primary type of the query")

    intent: str = Field(
        description="What the user wants to know (e.g., 'find candidates', 'get education details', 'compare candidates')"
    )

    entities: dict = Field(
        description="Extracted entities: names, skills, companies, locations, degrees, etc."
    )

    filters: dict = Field(
        description="Structured filters: min_experience, max_experience, etc."
    )

    search_strategy: Literal["sql_first", "vector_first", "hybrid", "sql_only"] = Field(
        description="Recommended search strategy based on query analysis"
    )

    confidence: float = Field(
        description="Confidence in the analysis (0.0 to 1.0)", ge=0.0, le=1.0
    )

    reasoning: str = Field(
        description="Brief explanation of why this strategy was chosen"
    )
    is_refinement: bool = Field(
    description="True if this query refines/filters previous results, False if it's a new search",
    default=False
    )


# ============= Initialize LLM =============
from dotenv import load_dotenv

load_dotenv()
# API key loaded from .env file

llm = ChatGroq(
    model="llama-3.3-70b-versatile",
    temperature=0.1,  # Low temperature for structured analysis
    max_tokens=4096,
)

answer_llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0.1, max_tokens=4096)


# ============= Agent Nodes =============


def summarize_old_messages(messages: list, llm) -> str:
    """
    Summarize old chat messages to preserve context while reducing token usage

    Args:
        messages: List of old message dicts to summarize
        llm: The LLM instance to use for summarization

    Returns:
        A concise summary string
    """
    # Format messages for summarization
    conversation_text = ""
    for msg in messages:
        role = msg.get("role", "unknown")
        content = msg.get("content", "")
        conversation_text += f"{role.upper()}: {content}\n\n"

    summarization_prompt = f"""You are an expert conversation summarizer for a resume search system. Your task is to create ultra-concise summaries that preserve ALL critical information.

CRITICAL RULES:
1. Extract and preserve ALL candidate names mentioned (with resume IDs if available)
2. Preserve ALL search criteria: skills, experience years, locations, companies, education
3. Preserve search refinements: "increased experience from 3 to 5 years", "added Python skill"
4. Use abbreviations: "exp" for experience, "yrs" for years, "ML" for Machine Learning
5. Use compact formats: "Python, Java, AWS" not "Python and Java and AWS"
6. Maximum 80 words - be extremely compact
7. Focus on FACTS, not conversational fluff

FEW-SHOT EXAMPLES:

Example 1:
USER: Find Python developers with more than 5 years experience
AGENT: Found 1 candidate: RATISH NAIR (resume_id: ccded4e6-ed27-4924-95b1-c2e312edc612) with 20 years experience, Python skills
USER: Show me their projects
AGENT: Displayed 15 projects for RATISH NAIR including Generative AI, RAG systems, MLOps
USER: What about their education?
AGENT: RATISH NAIR has B.Tech in Electronics & Communication from CUSAT (2000-2004)

SUMMARY:
Searched Python devs >5yrs exp. Found RATISH NAIR (ID: ccded4e6-ed27-4924-95b1-c2e312edc612, 20yrs exp). Discussed: 15 projects (Gen AI, RAG, MLOps), education (B.Tech Electronics CUSAT 2000-2004).

Example 2:
USER: Find candidates with Machine Learning experience in Bangalore
AGENT: Found 3 candidates in Bangalore with ML skills
USER: Show only those with 3+ years
AGENT: Narrowed to 2 candidates: Shubham Baghel, Anshika Chaudhary
USER: What companies did Shubham work at?
AGENT: Shubham worked at: Google, Microsoft, TCS

SUMMARY:
ML candidates in Bangalore, refined to 3+yrs: Shubham Baghel (ex-Google, Microsoft, TCS), Anshika Chaudhary.

Example 3:
USER: Find Java developers
AGENT: Found 8 Java developers
USER: Filter to those with AWS experience
AGENT: 5 candidates have both Java and AWS
USER: Show their current roles
AGENT: Current roles: 2 Senior Engineers, 2 Tech Leads, 1 Architect

SUMMARY:
Java devs (8 total) â†’ refined to Java+AWS (5 candidates) â†’ roles: 2 Sr Engineers, 2 Tech Leads, 1 Architect.

Example 4:
USER: Who has worked at Google?
AGENT: Found 4 candidates: Alex Kumar (resume_123), Priya Shah (resume_456), Raj Patel (resume_789), Sarah Lee (resume_012)
USER: Which ones know React?
AGENT: 2 candidates: Alex Kumar, Priya Shah have React skills
USER: Show Alex's education
AGENT: Alex Kumar: MS Computer Science Stanford 2018, BS MIT 2016

SUMMARY:
Ex-Google employees (4): Alex Kumar (ID: resume_123), Priya Shah (ID: resume_456), Raj Patel (ID: resume_789), Sarah Lee (ID: resume_012). Refined to React users: Alex, Priya. Alex edu: MS CS Stanford 2018, BS MIT 2016.

Now summarize this conversation using the same ultra-concise format:

CONVERSATION:
{conversation_text}

SUMMARY (max 80 words):"""

    try:
        response = llm.invoke(summarization_prompt)
        summary = response.content.strip()
        return summary
    except Exception as e:
        print(f"âš ï¸  Summarization failed: {e}")
        # Fallback: just list the topics
        return f"Previous conversation covered {len(messages)} messages about candidate search."
def analyze_query_node(state: AgentState) -> AgentState:
    """
    Node 1: Analyze user query to understand intent and plan strategy

    This is the BRAIN of the agent - decides what to do based on the question
    """

    # ============= BUILD CHAT CONTEXT =============
    chat_context = ""
    previous_candidate_ids = []
    most_recent_candidates = []  # âœ… Track MOST RECENT with names AND IDs

    if state["chat_history"]:
        history = state["chat_history"]

        # Extract candidates from chat history (process NEWEST first)
        for msg in reversed(history):
            if msg.get("role") == "agent":
                msg_ids = msg.get("candidate_ids", [])
                msg_names = msg.get("candidate_names", [])
                
                # Track MOST RECENT candidates (from last agent response)
                if not most_recent_candidates and msg_ids:
                    # Pair names with IDs
                    for i, cid in enumerate(msg_ids):
                        name = msg_names[i] if i < len(msg_names) else "Unknown"
                        most_recent_candidates.append({"id": cid, "name": name})
                
                # Collect all candidate IDs
                previous_candidate_ids.extend(msg_ids)

        # Remove duplicates from all IDs
        previous_candidate_ids = list(dict.fromkeys(previous_candidate_ids))

        # Check if we need to summarize (more than 10 messages)
        if len(history) > 10:
            # Split into old (to summarize) and recent (keep full)
            old_messages = history[:-8]  # All except last 8
            recent_history = history[-8:]  # Last 8 messages
            
            print(f"   ğŸ“š Summarizing {len(old_messages)} old messages...")
            summary = summarize_old_messages(old_messages, llm)
            
            chat_context = f"CONVERSATION SUMMARY (older messages):\n{summary}\n\n"
            chat_context += "RECENT CONVERSATION (last 8 messages):\n"
            
            # Format recent messages
            for i, msg in enumerate(recent_history, 1):
                role = msg.get("role", "unknown")
                content = msg.get("content", "")
                chat_context += f"{i}. {role.upper()}: {content}\n"
        else:
            # Short conversation, use all messages
            recent_history = history
            chat_context = "CONVERSATION HISTORY:\n"
            
            # Format all messages
            for i, msg in enumerate(recent_history, 1):
                role = msg.get("role", "unknown")
                content = msg.get("content", "")
                chat_context += f"{i}. {role.upper()}: {content}\n"

        # âœ… CRITICAL: Show MOST RECENT candidates with names for "these/those" pronoun resolution
        if most_recent_candidates:
            chat_context += f"\nğŸ“Œ MOST RECENT CANDIDATES (use these for 'these'/'those'/'them' pronouns):\n"
            for i, cand in enumerate(most_recent_candidates[:10], 1):
                chat_context += f"   {i}. {cand['name']} (ID: {cand['id']})\n"
        
        # Show all discussed candidates if different from most recent
        if len(previous_candidate_ids) > len(most_recent_candidates):
            other_ids = [cid for cid in previous_candidate_ids if cid not in [c['id'] for c in most_recent_candidates]]
            if other_ids:
                chat_context += f"\nğŸ“‹ OTHER CANDIDATES (earlier in conversation): {', '.join(other_ids[:5])}\n"
    # ============= ORIGINAL PROMPT (UNCHANGED) =============
    analysis_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """You are an intelligent recruitment assistant that helps recruiters find, analyze, and evaluate candidates strictly from a provided resume database (search results).

You must operate ONLY on the information explicitly present in the resumes. 
You are a strict resume analyst â€” not a general-purpose AI.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PRONOUN RESOLUTION (CRITICAL!)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
**When the user uses pronouns like "his", "her", "their", "this candidate", "the candidate":**
1. Look at the CONVERSATION HISTORY to find which candidate was most recently discussed
2. Extract that candidate's name and add it to the "entities" â†’ "names" field
3. Examples:
   - User asks: "Show me candidates with Python"
   - Agent responds with info about "John Smith"
   - User asks: "What is his education?"
   - YOU MUST extract: entities.names = ["John Smith"]
   
**IMPORTANT:** If pronouns are used but no previous candidate is clear, use an empty names list.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CORE RESPONSIBILITIES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Answer questions using ONLY the provided resume/search results
- Be specific, factual, and concise
- Use exact candidate names, skills, job titles, projects, tools, and years of experience as stated
- Highlight the most relevant candidates for the recruiter's query
- When comparing candidates, present a clear, structured comparison
- When recommending candidates, explain precisely WHY each candidate is suitable using resume evidence

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CRITICAL RULES (NON-NEGOTIABLE)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- NEVER mix up information between different candidates
- ALWAYS attribute skills, experience, projects, and achievements to the correct candidate
  â€¢ Example: "Candidate Rahul Sharma has 5 years of backend experience in Java"
- If multiple candidates are involved, keep their information clearly separated
- If information is missing, unclear, or not present in a candidate's resume, explicitly state:
  â†’ "Not specified in this candidate's resume"
- NEVER infer, guess, assume, or hallucinate details
- NEVER merge information from multiple resumes into one candidate

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ACHIEVEMENTS vs PROJECTS/WORK
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
**CRITICAL DISTINCTION:**
- **ACHIEVEMENTS, CERTIFICATIONS & OTHER INFORMATION** section contains:
  â€¢ Awards, honors, recognitions
  â€¢ Professional certifications (AWS, Google Cloud, etc.)
  â€¢ Coding platform profiles (Codeforces, LeetCode ratings)
  â€¢ Publications, patents
  â€¢ Volunteer work, extracurriculars
  â€¢ Languages spoken, hobbies, interests
  
- **Projects** section contains:
  â€¢ Technical projects built by the candidate
  â€¢ Descriptions, technologies used, role
  
- **Work Experience** section contains:
  â€¢ Job roles, companies, responsibilities
  
**When asked about "achievements", ONLY refer to the "ACHIEVEMENTS, CERTIFICATIONS & OTHER INFORMATION" section.**
**DO NOT list projects or work experience as achievements unless explicitly stated as achievements in that section.**

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RESPONSE FORMAT GUIDELINES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Use a clear structure with bullet points or numbered lists when helpful
- Mention candidate names prominently
- Include only relevant skills and experience for the query
- Keep responses concise but informative
- Maintain a professional, recruiter-focused tone
- Provide contact information ONLY if explicitly requested

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA TRUST POLICY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Trust ONLY the provided resumes/search results
- Accuracy, attribution, and candidate separation are mandatory
- If uncertain, say "Not specified" rather than making assumptions
"""),
            (
                "user",
                """{chat_context}

CURRENT QUERY: {query}

INSTRUCTIONS:
1. If query uses pronouns (his/her/their/these/those/them), look at ğŸ“Œ MOST RECENT CANDIDATES section
2. "these 2 candidates" = the first 2 from MOST RECENT CANDIDATES list
3. "his/her" = the single candidate from MOST RECENT CANDIDATES (if only 1)
4. Extract candidate names and add to entities.names
5. For 'X developer' queries, extract X as a skill (not job title)
6. Analyze the query and provide structured output""",
            ),
        ]
    )

    chain = analysis_prompt | llm.with_structured_output(QueryAnalysis)

    try:
        analysis = chain.invoke(
            {
                "query": state["query"],  # âœ… Original query unchanged
                "chat_context": chat_context,  # âœ… Context passed separately
            }
        )

        state["query_analysis"] = analysis.model_dump()
        state["search_strategy"] = analysis.search_strategy
        state["sql_filters"] = analysis.filters

        print(f"\nğŸ§  QUERY ANALYSIS:")
        print(f"   Type: {analysis.query_type}")
        print(f"   Strategy: {analysis.search_strategy}")
        print(f"   Reasoning: {analysis.reasoning}")
        print(f"   Confidence: {analysis.confidence:.2f}")
        if chat_context:
            print(
                f"   ğŸ“œ Used conversation context ({len(state['chat_history'])} messages)"
            )

    except Exception as e:
        print(f"âš ï¸  Analysis failed: {e}. Defaulting to hybrid search.")
        state["query_analysis"] = {"error": str(e)}
        state["search_strategy"] = "hybrid"
        state["sql_filters"] = {}

    return state


def sql_filter_node(state: AgentState) -> AgentState:
    """
    Node 2: Execute SQL filtering based on extracted entities

    Only runs if strategy requires SQL (sql_first, sql_only, hybrid)
    """

    if state["search_strategy"] == "vector_first":
        print("\nğŸ“Š SKIPPING SQL (vector-first strategy)")
        state["candidate_ids"] = []
        return state

    print("\nğŸ“Š EXECUTING SQL FILTERS...")

    filters = state["sql_filters"]
    analysis = state["query_analysis"]

    # Build SQL query dynamically
    import os

    db_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "resumes.db"
    )
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    where_clauses = []
    params = []

    # âœ… NEW: Candidate IDs filter (for follow-up queries) - CHECK THIS FIRST!
    if filters.get("candidate_ids"):
        candidate_id_list = filters["candidate_ids"]
        placeholders = ",".join("?" * len(candidate_id_list))
        where_clauses.append(f"resume_id IN ({placeholders})")
        params.extend(candidate_id_list)
        print(
            f"   ğŸ¯ Follow-up query: Filtering to {len(candidate_id_list)} previously discussed candidates"
        )

    # Extract name from entities (only if no candidate_ids filter)
    if not filters.get("candidate_ids"):
        entities = analysis.get("entities", {})
        if entities.get("names"):
            name = entities["names"][0]
            # Split name into parts to match more precisely
            # For "Shubham Baghel" â†’ match both "Shubham" AND "Baghel"
            name_parts = name.strip().split()
            if len(name_parts) >= 2:
                # Multi-part name: match all parts (prevents partial matches)
                name_conditions = []
                for part in name_parts:
                    name_conditions.append("candidate_name LIKE ?")
                    params.append(f"%{part}%")
                where_clauses.append(f"({' AND '.join(name_conditions)})")
                print(f"   ğŸ¯ Name filter: Matching all parts of '{name}'")
            else:
                # Single name: use simple LIKE
                where_clauses.append("candidate_name LIKE ?")
                params.append(f"%{name}%")

    # Experience filters
    if filters.get("min_experience"):
        where_clauses.append("total_experience_years >= ?")
        params.append(filters["min_experience"])

    if filters.get("max_experience"):
        where_clauses.append("total_experience_years <= ?")
        params.append(filters["max_experience"])

    # Location filter
    if filters.get("location"):
        where_clauses.append("location LIKE ?")
        params.append(f"%{filters['location']}%")

    # Skills filter (single column now)
    if filters.get("required_skills"):
        skill_conditions = []
        for skill in filters["required_skills"]:
            skill_conditions.append("skills LIKE ?")
            params.append(f"%{skill}%")
        where_clauses.append(f"({' OR '.join(skill_conditions)})")

    # Job title filter (searches in work_experience JSON and current_role)
    if filters.get("job_title"):
        job_title = filters["job_title"]
        # Search in both work_experience JSON and current_role column
        where_clauses.append("(work_experience LIKE ? OR current_role LIKE ?)")
        params.append(f"%{job_title}%")
        params.append(f"%{job_title}%")

    # Company filter (searches in work_experience JSON)
    if filters.get("company"):
        where_clauses.append("work_experience LIKE ?")
        params.append(f"%{filters['company']}%")

    # Current role filter (exact match in current_role column)
    if filters.get("current_role"):
        where_clauses.append("current_role LIKE ?")
        params.append(f"%{filters['current_role']}%")

    # Build final query
    sql = "SELECT resume_id FROM parsed_resumes"
    if where_clauses:
        sql += " WHERE " + " AND ".join(where_clauses)

    print(f"   SQL: {sql}")
    print(f"   Params: {params}")

    cursor.execute(sql, params)
    results = cursor.fetchall()
    conn.close()

    candidate_ids = [row[0] for row in results]
    state["candidate_ids"] = candidate_ids

    print(f"   âœ… Found {len(candidate_ids)} candidates via SQL")

    # If no results and this was a sql_only query, mark for LLM SQL retry
    if not candidate_ids and state["search_strategy"] == "sql_only":
        state["use_llm_sql"] = True

    return state


def llm_sql_generation_node(state: AgentState) -> AgentState:
    """
    Node 2.5: Generate SQL query using LLM for complex queries

    Invoked on retry when predefined SQL filters don't work.
    LLM gets the schema and generates appropriate SQL.
    """

    # Only run if use_llm_sql flag is set
    if not state.get("use_llm_sql"):
        return state

    print("\nğŸ¤– GENERATING SQL WITH LLM...")

    # Database schema information for the LLM
    schema_info = """
    Table: parsed_resumes
    Columns:
    - resume_id (TEXT, PRIMARY KEY)
    - document_id (TEXT)
    - candidate_name (TEXT)
    - email (TEXT)
    - phone (TEXT)
    - location (TEXT)
    - total_experience_years (REAL) - Number of years of work experience
    - current_role (TEXT)
    - skills (TEXT) - JSON array of all skills
    - work_experience (TEXT) - JSON array of jobs with company, role, duration
    - education (TEXT) - JSON array of degrees
    """

    sql_generation_prompt = f"""
You are a SQL expert. Generate a SQLite query to find resume_ids matching this user question.

DATABASE SCHEMA:
{schema_info}

USER QUESTION: {state["query"]}

RULES:
1. ONLY return the SQL query, nothing else
2. Always SELECT resume_id
3. Use LIKE for text matching with % wildcards
4. For experience, use total_experience_years (it's a number)
5. For skills, search in the skills column (JSON array as text)

SQL QUERY:
"""

    try:
        response = llm.invoke(sql_generation_prompt)
        generated_sql = response.content.strip()

        # Clean up the SQL (remove markdown code blocks if present)
        if "```" in generated_sql:
            generated_sql = generated_sql.split("```")[1]
            if generated_sql.startswith("sql"):
                generated_sql = generated_sql[3:]
            generated_sql = generated_sql.strip()

        print(f"   Generated SQL: {generated_sql}")

        # Execute the generated SQL
        import os

        db_path = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "resumes.db"
        )
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        cursor.execute(generated_sql)
        results = cursor.fetchall()
        conn.close()

        candidate_ids = [row[0] for row in results]
        state["candidate_ids"] = candidate_ids
        state["llm_generated_sql"] = generated_sql

        print(f"   âœ… LLM SQL found {len(candidate_ids)} candidates")

    except Exception as e:
        print(f"   âŒ LLM SQL failed: {str(e)[:100]}")
        state["candidate_ids"] = []

    return state


def vector_search_node(state: AgentState) -> AgentState:
    """
    Node 3: Execute vector search with optional metadata filtering

    For sql_only strategy: Skip vector search and use SQL results directly
    For other strategies: Use vector search with SQL filters
    """

    # For sql_only strategy with SQL results, skip vector search entirely
    if state["search_strategy"] == "sql_only" and state["candidate_ids"]:
        print("\nğŸ” SKIPPING VECTOR SEARCH (sql_only strategy)")
        print(f"   Using {len(state['candidate_ids'])} candidates from SQL directly")

        # Create mock search results with just the resume_ids from SQL
        state["search_results"] = {
            "ids": [[f"{rid}__direct" for rid in state["candidate_ids"]]],
            "metadatas": [[{"resume_id": rid} for rid in state["candidate_ids"]]],
            "documents": [["" for _ in state["candidate_ids"]]],
            "distances": [[0.0 for _ in state["candidate_ids"]]],
        }
        return state

    print("\nğŸ” EXECUTING VECTOR SEARCH...")

    # Use vector store directly (not HybridResumeSearch) to apply ChromaDB filters correctly
    from vectorstore.chroma_store import ResumeVectorStore

    vector_store = ResumeVectorStore()

    # Prepare metadata filter if we have candidate IDs from SQL
    vector_filters = None
    if state["candidate_ids"]:
        vector_filters = {"resume_id": {"$in": state["candidate_ids"]}}
        print(f"   Filtering to {len(state['candidate_ids'])} candidates from SQL")

    # Execute search directly on vector store with proper filter format
    results = vector_store.search(
        query=state["vector_query"], top_k=10, filters=vector_filters
    )

    state["search_results"] = results

    print(f"   âœ… Found {len(results.get('ids', [[]])[0])} vector matches")

    return state


def enrich_results_node(state: AgentState) -> AgentState:
    """
    Node 4: Fetch full resume data from SQLite and deduplicate
    
    Handles same resume appearing multiple times (different chunks) from vector search results
    """

    print("\nğŸ’ ENRICHING RESULTS...")

    if not state["search_results"] or not state["search_results"].get("metadatas"):
        state["final_results"] = []
        state["should_retry"] = True
        return state

    # Deduplicate by resume_id (keep ALL chunks for each resume)
    resume_chunks = {}  # {resume_id: [list of chunks]}
    metadatas = state["search_results"]["metadatas"][0]
    documents = state["search_results"]["documents"][0]
    distances = state["search_results"]["distances"][0]

    for meta, doc, dist in zip(metadatas, documents, distances):
        resume_id = meta.get("resume_id")
        
        if resume_id not in resume_chunks:
            resume_chunks[resume_id] = []
        
        # Store ALL chunks for this resume (not just highest scoring)
        resume_chunks[resume_id].append({
            "chunk_type": meta.get("chunk_type"),
            "chunk_text": doc,
            "distance": dist,
            "metadata": meta
        })

    unique_ids = list(resume_chunks.keys())
    print(
        f"   Deduplicated: {len(metadatas)} chunks â†’ {len(unique_ids)} unique resumes"
    )

    # Fetch full data from SQLite
    import os

    db_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "resumes.db"
    )
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()

    placeholders = ",".join("?" * len(unique_ids))
    query = f"""
        SELECT 
            pr.*,
            d.raw_text
        FROM parsed_resumes pr
        LEFT JOIN documents d ON pr.document_id = d.document_id
        WHERE pr.resume_id IN ({placeholders})
    """

    cursor.execute(query, unique_ids)
    rows = cursor.fetchall()
    conn.close()

    final_results = []
    for row in rows:
        resume_dict = dict(row)
        resume_id = resume_dict['resume_id']
        
        # âœ… NEW: Add matched chunks to resume data
        resume_dict['matched_chunks'] = resume_chunks.get(resume_id, [])
        
        final_results.append(resume_dict)

    state["final_results"] = final_results

    print(f"   âœ… Enriched {len(final_results)} resumes with full data + matched chunks")

    # Decide if we should retry
    state["should_retry"] = len(final_results) == 0

    return state


def generate_answer_node(state: AgentState) -> AgentState:
    """
    Node 5: Generate natural language answer using LLM

    Uses existing answer generation function
    """

    print("\nğŸ¤– GENERATING ANSWER...")

    if not state["final_results"]:
        state["answer"] = (
            "I couldn't find any candidates matching your criteria. Try:\n- Broadening your search terms\n- Removing some filters\n- Checking for typos in names or skills"
        )
        return state

    # âœ… CRITICAL FIX: Limit to top 5 candidates to prevent hallucinations
    # When LLM gets 10+ candidates, it starts mixing up information
    results_to_use = state["final_results"][:5]
    
    if len(state["final_results"]) > 5:
        print(f"   âš ï¸  Limiting results: {len(state['final_results'])} candidates â†’ top 5")
    
    # âœ… ADDITIONAL FIX: If query is about ONE specific candidate, filter to just that candidate
    # Example: "show me projects of shubham baghel" or "his projects"
    query_lower = state["query"].lower()
    is_specific_query = any(keyword in query_lower for keyword in [
        "their", "his", "her", "this candidate", "the candidate"
    ])
    
    # If asking about specific candidate AND we have candidate_ids filter (follow-up)
    if is_specific_query and state["sql_filters"].get("candidate_ids"):
        candidate_id = state["sql_filters"]["candidate_ids"][0]
        # Filter to only this candidate
        results_to_use = [r for r in results_to_use if r["resume_id"] == candidate_id]
        print(f"   ğŸ¯ Specific query detected: Showing ONLY candidate {candidate_id}")
    
    # OR if name was specified in query, filter to exact name match
    elif state["query_analysis"].get("entities", {}).get("names"):
        specified_name = state["query_analysis"]["entities"]["names"][0].lower()
        # Check if only 1 candidate matches this name
        name_matches = [r for r in results_to_use if specified_name in r.get("candidate_name", "").lower()]
        if len(name_matches) == 1:
            results_to_use = name_matches
            print(f"   ğŸ¯ Single candidate query: Showing ONLY {name_matches[0]['candidate_name']}")
    
    answer = generate_answer(
        query=state["query"], search_results=results_to_use
    )

    state["answer"] = answer

    print(f"   âœ… Answer generated ({len(answer)} chars)")

    return state


def should_retry_node(state: AgentState) -> Literal["retry", "end"]:
    """
    Router: Decide whether to retry with different strategy

    On retry, enable LLM SQL generation for complex queries
    """

    if state["should_retry"] and state.get("retry_count", 0) < 1:
        print("\nğŸ”„ NO RESULTS - RETRYING WITH LLM SQL GENERATION...")
        state["retry_count"] = state.get("retry_count", 0) + 1
        state["use_llm_sql"] = True  # Enable LLM SQL on retry

        # Switch strategy to vector_first if SQL failed
        if state["search_strategy"] in ["sql_first", "sql_only"]:
            state["search_strategy"] = "vector_first"
        else:
            state["search_strategy"] = "hybrid"

        return "retry"

    return "end"


# ============= Build Graph =============


def create_intelligent_agent() -> StateGraph:
    """
    Build the LangGraph workflow for intelligent resume search

    Flow:
    1. Analyze query â†’ 2. SQL filter â†’ 2.5. LLM SQL (if needed) â†’ 3. Vector search â†’
    4. Enrich results â†’ 5. Generate answer â†’ Retry if needed
    """

    workflow = StateGraph(AgentState)

    # Add nodes
    workflow.add_node("analyze_query", analyze_query_node)
    workflow.add_node("sql_filter", sql_filter_node)
    workflow.add_node("llm_sql_generation", llm_sql_generation_node)  # NEW
    workflow.add_node("vector_search", vector_search_node)
    workflow.add_node("enrich_results", enrich_results_node)
    workflow.add_node("generate_answer", generate_answer_node)

    # Define edges
    workflow.set_entry_point("analyze_query")
    workflow.add_edge("analyze_query", "sql_filter")
    workflow.add_edge("sql_filter", "llm_sql_generation")  # Try LLM SQL if needed
    workflow.add_edge("llm_sql_generation", "vector_search")
    workflow.add_edge("vector_search", "enrich_results")
    workflow.add_edge("enrich_results", "generate_answer")

    # Conditional retry logic
    workflow.add_conditional_edges(
        "generate_answer",
        should_retry_node,
        {"retry": "sql_filter", "end": END},  # Go back and try different strategy
    )

    return workflow.compile()


# ============= Easy-to-Use Interface =============


class ResumeIntelligenceAgent:
    """
    Main agent interface - handles everything automatically

    Usage:
        agent = ResumeIntelligenceAgent()
        answer = agent.query("Find Python developers with 5+ years")
    """

    def __init__(self):
        self.graph = create_intelligent_agent()

    def query(
        self, user_query: str, session_id: str = None, verbose: bool = True
    ) -> dict:
        """
        Process a query and return natural language answer

        The agent will automatically:
        - Load conversation history (if session_id provided)
        - Save user message to database
        - Analyze the query to understand intent
        - Choose the best search strategy (SQL, vector, or hybrid)
        - Execute the search with appropriate filters
        - Deduplicate results
        - Generate a helpful answer
        - Save agent response to database
        - Retry with different strategy if needed

        Args:
            user_query: The user's question
            session_id: Optional session ID to continue a conversation
            verbose: Whether to print debug information

        Returns:
            Dict with keys: 'answer', 'session_id', 'candidate_ids'
        """
        # Import chat manager
        from chat.chat_manager import (
            create_chat_session,
            save_user_message,
            save_agent_message,
            load_chat_history,
        )

        # Create or use existing session
        if not session_id:
            session_id = create_chat_session(
                title=user_query[:50]
            )  # Use first 50 chars as title
            chat_history = []
        else:
            chat_history = load_chat_history(session_id, limit=10)

        # Save user message
        user_msg_id = save_user_message(session_id, user_query)

        if verbose:
            print("=" * 70)
            print(f"ğŸ“ USER QUERY: {user_query}")
            if chat_history:
                print(
                    f"ğŸ’¬ CONTINUING SESSION: {session_id} ({len(chat_history)} previous messages)"
                )
            else:
                print(f"ğŸ†• NEW SESSION: {session_id}")
            print("=" * 70)

        # Initialize state with chat history
        initial_state = {
            "query": user_query,
            "query_analysis": {},
            "search_strategy": "hybrid",
            "sql_filters": {},
            "vector_query": user_query,
            "candidate_ids": [],
            "search_results": [],
            "final_results": [],
            "answer": "",
            "should_retry": False,
            "retry_count": 0,
            "use_llm_sql": False,
            "llm_generated_sql": "",
            "chat_history": chat_history,  # âœ… Load from database
        }

        # Run the graph
        final_state = self.graph.invoke(initial_state)

        # Extract results
        answer = final_state["answer"]
        final_results = final_state.get("final_results", [])
        candidate_ids = [result.get("resume_id") for result in final_results]
        candidate_names = [result.get("candidate_name") for result in final_results]  # âœ… NEW
        query_analysis = final_state.get("query_analysis", {})
        search_strategy = final_state.get("search_strategy", "unknown")

        # Save agent message with names AND IDs
        agent_msg_id = save_agent_message(
            session_id=session_id,
            content=answer,
            candidate_ids=candidate_ids,
            candidate_names=candidate_names,  # âœ… NEW
            search_type=search_strategy,
            query_analysis=query_analysis,
        )

        if verbose:
            print("\n" + "=" * 70)
            print("âœ… FINAL ANSWER:")
            print("=" * 70)
            print(answer)
            print("=" * 70)
            print(f"ğŸ’¾ Saved to session: {session_id}")
            if candidate_ids:
                print(f"ğŸ‘¥ Returned {len(candidate_ids)} candidates")
            print("=" * 70)

        return {
            "answer": answer,
            "session_id": session_id,
            "candidate_ids": candidate_ids,
        }


# ============= Testing =============

if __name__ == "__main__":
    agent = ResumeIntelligenceAgent()

    print("\nğŸ¤– CONVERSATIONAL AGENT TEST\n")

    # First query - creates new session
    print("â”" * 70)
    print("TEST 1: New conversation")
    print("â”" * 70)
    result1 = agent.query("Find Python developers with more than 5 years experience")
    session_id = result1["session_id"]

    print(f"\nğŸ“Š Result: Found {len(result1['candidate_ids'])} candidates")
    print(f"ğŸ†” Session ID: {session_id}")

    # Follow-up query - uses context
    print("\n" + "â”" * 70)
    print("TEST 2: Follow-up question (should understand 'their')")
    print("â”" * 70)
    result2 = agent.query("Show me their projects", session_id=session_id)

    print(f"\nğŸ“Š Result: {len(result2['candidate_ids'])} candidates")

    # Another follow-up
    print("\n" + "â”" * 70)
    print("TEST 3: Another follow-up (should use same candidates)")
    print("â”" * 70)
    result3 = agent.query("What are their skills?", session_id=session_id)

    print(f"\nğŸ“Š Result: {len(result3['candidate_ids'])} candidates")

    print("\nâœ… CONVERSATIONAL TEST COMPLETE!")
